---
title: "Economertics"
author: "Fotso Tenku"
date: "17 01 2020"
output: html_document
---

## Problem 1

```{r}
Anscombe.<-read.table("Anscombe.txt",header=TRUE)
head(Anscombe.)
Y=Anscombe.$education     
X1=Anscombe.$income      
X2=Anscombe.$young 
X3=Anscombe.$urban   
```

```{r}
par(mfrow = c(2, 2)) 
plot(X1,Y,main="Education against income")
plot(X2,Y,main="Education against young")
plot(X3,Y,main="Education against urban")
```

## fitted model

```{r}
set.seed(6810782)
Reg1=lm(Y~X1)
Reg2=lm(Y~X2)
Reg3=lm(Y~X3)
Reg4=lm(Y~X1+X2)
Reg5=lm(Y~X1+X3)
Reg6=lm(Y~X2+X3)
Reg7=lm(Y~X1+X2+X3)

summary(Reg1)
summary(Reg2)
summary(Reg3)
summary(Reg4)
summary(Reg5)
summary(Reg6)
summary(Reg7)
```

```{r}
par(mfrow=c(2,2))
plot(X1,Y,main="Education against income")
abline(Reg1,col="blue")
plot(X2,Y,main="Education against young")
abline(Reg2,col="blue") 
plot(X3,Y,main="Education against urban")
abline(Reg3,col="blue")
```

## d)calculate the adjusted R squared

```{r}
summary(Reg1)$adj.r.squared
summary(Reg2)$adj.r.squared
summary(Reg3)$adj.r.squared
summary(Reg4)$adj.r.squared
summary(Reg5)$adj.r.squared
summary(Reg6)$adj.r.squared
summary(Reg7)$adj.r.squared
```

## Question E)Calculate the 95% and 99% confindence intervals for all parameters? of the best model chosen in d

```{r}
beta_1 = coef(Reg7)[1]
beta_2 = coef(Reg7)[2]
beta_3 = coef(Reg7)[3]
beta_4 = coef(Reg7)[4]


t_0.025 = qt(0.975, df = 47)
t_0.005 = qt(0.995, df = 47)				

se_beta_1 = coef(summary(Reg7))[1,"Std. Error"]
se_beta_2 = coef(summary(Reg7))[2,"Std. Error"]
se_beta_3 = coef(summary(Reg7))[3,"Std. Error"]
se_beta_4 = coef(summary(Reg7))[4,"Std. Error"]

CI1 = c(beta_1 - t_0.025*se_beta_1, beta_1 + t_0.025*se_beta_1)
CI2 = c(beta_2 - t_0.025*se_beta_2, beta_2 + t_0.025*se_beta_2)
CI3 = c(beta_3 - t_0.025*se_beta_3, beta_3 + t_0.025*se_beta_3)
CI4 = c(beta_4 - t_0.025*se_beta_4, beta_4 + t_0.025*se_beta_4)

CI11 = c(beta_1 - t_0.005*se_beta_1, beta_1 + t_0.005*se_beta_1)
CI21 = c(beta_2 - t_0.005*se_beta_2, beta_2 + t_0.005*se_beta_2)
CI31 = c(beta_3 - t_0.005*se_beta_3, beta_3 + t_0.005*se_beta_3)
CI41 = c(beta_4 - t_0.005*se_beta_4, beta_4 + t_0.005*se_beta_4)

names(CI1) = c("lower bound", "upper bound")
names(CI2) = c("lower bound", "upper bound")
names(CI3) = c("lower bound", "upper bound")
names(CI4) = c("lower bound", "upper bound")
names(CI11) = c("lower bound", "upper bound")
names(CI21) = c("lower bound", "upper bound")
names(CI31) = c("lower bound", "upper bound")
names(CI41) = c("lower bound", "upper bound")
```

## Bonuspunkt				

```{r}
CI1<-confint(Reg7,level=0.95)
CI1
CI2<-confint(Reg7,level=0.99)
CI2
```

## Question F) Use an F-test to test the joint significance of the slope parameters at ?? = 0.1,0.05,0.01 and state your null and alternative hypothesis. Also calculate the p-value of this F-tests. Interpret the slope coeficients of this model. Are your expectations from a. met

```{r}
summary(Reg7)
F1<-qf(0.9,df1=3,df2=47)
F2<-qf(0.95,df1=3,df2=47)
F3<-qf(0.99,df1=3,df2=47)
Fisher=summary(Reg7)$fstatistic

#calculate p value
1 - pf(summary(Reg7)$fstatistic[1], 3, 47)
```

## Problem2

```{r}
# Run this code (please do not edit the lines below)
Data = read.table("W4479 - Problem 2 Data.txt",header=TRUE)
set.seed(6810782)
choose = sample(1:50, 3, replace = TRUE)
Y1 = Data[, choose[1]]
Y2 = Data[, choose[1] + 50]
Y3 = Data[, choose[1] + 100]
X1 = Data[, 151]
X2 = Data[, 152]
X3 = Data[, 153]

rm(list = c("Data"))
```

## Data transformation

```{r}
# (X1, Y1)
X = log(X1)				
X_inv = 1/X1
Y = log(Y1)

#(X2,Y2)

Y.2=log(Y2)
X.2=log(X2)
X_inv.2=1/X2
```

## Graphical analysis

```{r}

#(X1,Y1)

par(mfrow = c(2,3))				
plot(X1, Y1, main = "Model")		
plot(X, Y1, main = "lin-log")
plot(X1, Y, main = "log-lin")
plot(X_inv, Y1, main = "reciprocal")
plot(X,Y,main ="log-linear")
plot(X_inv,Y,main = "log-reciprocal")
```

```{r}
#(X2,Y2)
par(mfrow= c(2,3))
plot(X2,Y2,main ="model2")
plot(X.2,Y.2,xlab = "logX2",ylab = "logY2")
title("Log-linear")
plot(X_inv.2,Y2,xlab = "X_inv.2",ylab = "Y2")
title("Reciprocal")
plot(X.2,Y2,xlab = "logx2",ylab = "Y2")
title("lin-log")
plot(X2,Y.2,xlab = "X2",ylab="logY2")
title("log-lin")
plot(X_inv.2,Y.2,xlab = "X_inv.2",ylab = "logY2")
title("Log reciprocal")
```

## RSS analysis

```{r}
# (X1,Y1)

reg_1 = lm(Y1~X)
reg_2 = lm(Y~X1)
reg_3 = lm(Y1~X_inv)
reg_4 = lm(Y~X)
reg_5 = lm(Y~X_inv)

#fitted Values

Y_1 = reg_1$fitted.values
Y_2 = exp(reg_2$fitted.values)
Y_3 = reg_3$fitted.values
Y_4 = exp(reg_4$fitted.values)
Y_5 = exp(reg_5$fitted.values)



res_1 = Y1 - Y_1			
res_2 = Y1 - Y_2
res_3 = Y1 - Y_3
res_4 = Y1 - Y_4
res_5 = Y1 - Y_5

RSS = function(res) { sum(res^2) }
RSS(res_1)				
RSS(res_2)
RSS(res_3)
RSS(res_4)
RSS(res_5)
```

```{r}
#(Y2,X2)


reg1=lm(Y.2~X.2)
reg2=lm(Y2~X_inv.2)
reg3=lm(Y2~X.2)
reg4=lm(Y.2~X2)
reg5=lm(Y.2~X_inv.2)


Y.1 = exp(reg1$fitted.values)	
Y.2 = reg2$fitted.values
Y.3 = reg3$fitted.values
Y.4 = exp(reg4$fitted.values)
Y.5 = exp(reg5$fitted.values)

res.1 = Y2 - Y.1			 
res.2 = Y2 - Y.2
res.3 = Y2 - Y.3
res.4 = Y2 - Y.4
res.5 = Y2 - Y.5

RSS = function(res) { sum(res^2) }
RSS(res.1)				 
RSS(res.2)
RSS(res.3)
RSS(res.4)
RSS(res.5)
```

## b) plot the best Model

```{r}
par(mfrow = c(1,2))
plot(X,Y,xlab = "logX2",ylab = "logY2")
title("Log-linear")
abline(reg_4,col="red")
plot(X.2,Y2,xlab = "logx2",ylab = "Y2")
title("lin-log")
abline(reg3,col="red")
```

```{r}
ord = order(X1) 
X1 = X1[ord]
Y1=Y1[ord]
Y_4 = Y_4[ord] 
par(mfrow = c(1,2))
plot(X1,Y1,main = "model1")
lines(X1,Y_4,col="red")

ord2= order(X2)
X2= X2[ord2]
Y2=Y2[ord2]
Y.3 = Y.3[ord2]
plot(X2,Y2, main = "model2")
lines(X2,Y.3,col="red")
```

## c) Test the assumptions of normal distributed error term 

```{r}
# (X1,Y2)
n=length(X)
residuals = residuals(reg_4)
residuals=residuals(lm(Y~X))
Res_bar = mean(residuals)
sd = sqrt(mean((residuals - Res_bar)^2))	
c = residuals - Res_bar	
S = (1/n)*(sum(c^3))/(1/n*sum(c^2))^1.5	
K = (1/n*sum(c^4))/(1/n*sum(c^2))
```

```{r}
library(moments)					
S = skewness(residuals)
K = kurtosis(residuals)

# Jarque-Bera Test
JB = (n/6)*(S^2 + (K-3)^2/4)			
p = 1-pchisq(JB,2)				
p; JB
```

```{r}
par(mfrow = c(2,2))
plot(X, residuals, main = "Residual Plot")
hist(residuals)
qqnorm(residuals, ylab = "Residuals", xlab = "Quantiles of Standard Normal")
qqline(residuals,col="red")
```

```{r}
# (X2,Y2)
n=length(X.2)
residuals = residuals(reg3)
Res_bar = mean(residuals)
sd = sqrt(mean((residuals - Res_bar)^2))	
c = residuals - Res_bar	# centered values
S = (1/n)*(sum(c^3))/(1/n*sum(c^2))^1.5	
K = (1/n*sum(c^4))/(1/n*sum(c^2))^2		
library(moments)					
S = skewness(residuals)
K = kurtosis(residuals)

# Jarque-Bera Test
JB = (n/6)*(S^2 + (K-3)^2/4)			
p = 1-pchisq(JB,2)				
p; JB
```

```{r}
par(mfrow = c(2,2))
plot(X.2, residuals, main = "Residual Plot")
hist(residuals)
qqnorm(residuals, ylab = "Residuals", xlab = "Quantiles of Standard Normal")
qqline(residuals,col="red")

```

## d)calculate ß1, ß2 of (Y3,X3)

```{r}
model<-lm(Y3~X3)
A<-matrix(c(1,2,0.3,-1),nrow=2,ncol=2)
b<-matrix(c(model$coef[1],model$coef[2]),nrow=2,ncol=1)
solve(A,b)
```

## Problem 3

```{r}
#partition
set.seed(6810782)
Data<-read.table("engel.txt",header=TRUE)
ind<-sample(1:235,size=0.6*nrow(Data),replace=FALSE,prob=NULL)
Data_train<-Data[ind,]
Data_test<-Data[-ind,]

# Regression
reg_train<-lm(foodexp~income,data=Data_train)
summary(reg_train)
```

```{r}
par(mfrow=c(1,1))
plot(Data_train$income,Data_train$foodexp,main="Data")
abline(reg_train,col="red")
```

```{r}
reg_test<-lm(foodexp~income,data=Data_test)


#predict
predict<-predict(reg_train,newdata=Data_test)

plot(Data_test$income,Data_test$foodexp,main="Test")
abline(reg_test,col="red")
points(Data_test$income,predict,col="green")
lines(Data_test$income,predict,col="blue")
```

```{r}
# berechnen die Standard deviation
standard_deviation <- predict(reg_test, se.fit=TRUE)
standard_deviation<- data.frame(standard_deviation)
standard_deviation

#conf interval
conf<-predict(reg_test,interval="confidence")
conf<-data.frame(conf)
```

```{r}
#plot
plot(Data_test$income,Data_test$foodexp,main="Data_test")
abline(reg_train,col="red")
lines(Data_test$income,conf[,"lwr"], col="blue")
lines(Data_test$income,conf[,"upr"], col="blue")
```

